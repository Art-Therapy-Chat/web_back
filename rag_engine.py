# rag_engine.py
import torch
import json
from typing import List, Dict
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.documents import Document
from embeddings import vectorstore, cross_encoder


# ===============================================================
# 1) Multi Query Generator (OpenAI GPT-4o-mini)
# ===============================================================

class MultiQueryGenerator(BaseModel):
    queries: List[str] = Field(description="ê²€ìƒ‰ ì¿¼ë¦¬ ëª©ë¡")


class AdvancedQueryRewriter:
    def __init__(self, model_name="gpt-4o", temperature=0):
        self.llm = ChatOpenAI(
            model_name=model_name,
            temperature=temperature,
            max_tokens=1000
        )
        self.parser = JsonOutputParser(pydantic_object=MultiQueryGenerator)

        self.template = """
        ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ìš© ì¿¼ë¦¬ë¥¼ ì¬ì‘ì„±í•©ë‹ˆë‹¤.

        # ì§€ì¹¨
        1. ì•„ë˜ì˜ history ì— í¬í•¨ëœ ëª¨ë“  ì´ì „ ì§ˆì˜/ê²€ìƒ‰ë¬¸ì„œ/ë‹µë³€ì„ ì°¸ê³ í•˜ì—¬ ë” ì •í™•í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.
        2. í˜„ì¬ ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ê±°ë‚˜ ìƒëµëœ ê²½ìš°, history ë¥¼ ì°¸ê³ í•´ ë¬¸ë§¥ìƒ ì™„ì „í•œ ì¿¼ë¦¬ë¡œ ì¬êµ¬ì„±í•˜ì„¸ìš”.
        3. ì´ì „ ëŒ€í™”ê°€ ì—†ê±°ë‚˜ ê´€ë ¨ì´ ì—†ëŠ” ê²½ìš° í˜„ì¬ ì§ˆë¬¸ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
        4. ë°˜ë“œì‹œ ëª…í™•í•˜ê³  ê²€ìƒ‰ì— ì í•©í•œ ì¿¼ë¥¼ ìƒì„±í•˜ì„¸ìš”.
        6. ì¶œë ¥ì€ ì¬ìƒì„±ëœ ì¿¼ë¦¬ ë¬¸ìì—´ë§Œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ì£¼ì„ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        7. í˜„ì¬ ë¬¸ì¥ì´ ì—¬ëŸ¬ ì†ì„±ì„ í¬í•¨í•˜ê³  ìˆë‹¤ë©´, ê°ê°ì„ ë³„ë„ì˜ ì¿¼ë¦¬ë¡œ ë¶„ë¦¬í•˜ì„¸ìš”.
        8. ê° ì¿¼ë¦¬ëŠ” ë…ë¦½ì ìœ¼ë¡œ ë²¡í„° DBì—ì„œ ê²€ìƒ‰ë  ìˆ˜ ìˆë„ë¡ ì™„ì „í•˜ê³  ëª…í™•í•´ì•¼ í•©ë‹ˆë‹¤.
        10. ë¶„ë¦¬ëœ ì¿¼ë¦¬ë“¤ì„ í•©ì³¤ì„ ë•Œ ì›ë˜ ì¿¼ë¦¬ì˜ ì˜ë¯¸ë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë„ë¡ í•˜ì„¸ìš”.

        ì „ì²´ ëŒ€í™”: {history_text}
        ì‚¬ìš©ì ì§ˆë¬¸: {current_query}

        ì¶œë ¥ ì˜ˆì‹œ:
        {{
            "queries": ["ì¿¼ë¦¬1", "ì¿¼ë¦¬2"]
        }}

        ì˜ˆì‹œ :
        í˜„ì¬ ì§ˆë¬¸ì´ "ì„œìš¸ì€ ? ê·¸ë¦¬ê³  ë§›ì§‘ì€?"ì´ê³  ì´ì „ ëŒ€í™”ê°€ "í•œêµ­ì˜ ê´€ê´‘ì§€ ì¶”ì²œí•´ì¤˜"ë¼ë©´,
        {{{{
            "queries" : ["ì„œìš¸ì˜ ê´€ê´‘ì§€ ì¶”ì²œí•´ì¤˜", "ì„œìš¸ì˜ ë§›ì§‘ ì¶”ì²œí•´ì¤˜"]
        }}}}

        ë‹¨ì¼ ì¿¼ë¦¬ì¸ ê²½ìš°:
        {{{{
            "queries" : ["í•œêµ­ì˜ ê´€ê´‘ì§€ ì¶”ì²œí•´ì¤˜"]
        }}}}

        {format_instructions}
        """

        self.prompt = PromptTemplate(
            input_variables=["history_text", "current_query"],
            template=self.template,
        )

    def rewrite_query(self, history_text: str, current_query: str) -> List[str]:
        if not history_text.strip():
            history_text = "ëŒ€í™” ê¸°ë¡ ì—†ìŒ"

        format_instructions = self.parser.get_format_instructions()

        prompt = self.prompt.format(
            history_text=history_text,
            current_query=current_query,
            format_instructions=format_instructions
        )

        llm_response = self.llm.invoke(prompt).content

        try:
            data = json.loads(llm_response)
            return data.get("queries", [current_query])
        except:
            print("âš  JSON íŒŒì‹± ì‹¤íŒ¨, ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©")
            return [current_query]


# ===============================================================
# 2) Multi Query Retriever
# ===============================================================

class MultiQueryRetriever:
    def __init__(self, vectorstore, query_rewriter):
        self.vectorstore = vectorstore
        self.cross_encoder = cross_encoder
        self.history = []
        self.query_rewriter = query_rewriter

    def build_history_text(self):
        out = ""
        for h in self.history:
            out += f"[USER]\n{h['user_query']}\n"
            out += f"[REWRITTEN]\n{h['rewritten_queries']}\n"
            out += "[DOCS]\n"
            for d in h["retrieved_docs"]:
                out += f"- {d['content']}\n"
            out += f"[ANSWER]\n{h['final_answer']}\n"
            out += "-" * 30 + "\n"
        return out

    def retrieve(self, query: str, category: str, num_docs=3):

        history_text = self.build_history_text()
        rewritten_queries = self.query_rewriter.rewrite_query(history_text, query)

        print("ì¬ì‘ì„±ëœ ì¿¼ë¦¬:", rewritten_queries)

        all_docs = []
        seen = set()

        for q in rewritten_queries:
            docs = self.vectorstore.similarity_search(
                q,
                k=num_docs,
                filter={"category": category}
            )

            for d in docs:
                if d.page_content not in seen:
                    seen.add(d.page_content)
                    all_docs.append(d)

        # CrossEncoder ë¼ë­í‚¹ ì ìš©
        # -------------------------------
        if all_docs:
            pairs = [(query, d.page_content) for d in all_docs]
            scores = self.cross_encoder.predict(pairs)
            reranked = sorted(zip(all_docs, scores), key=lambda x: x[1], reverse=True)
            all_docs = [doc for doc, score in reranked]

        return all_docs, rewritten_queries


# ===============================================================
# 3) Fine-tuned Qwen2.5 HTP ëª¨ë¸ ê¸°ë°˜ RAG
# ===============================================================

# model.pyì˜ ì‹±ê¸€í†¤ ëª¨ë¸ ì¬ì‚¬ìš©
from model import _load_model

# RAG ì‘ë‹µ ìƒì„± í´ë˜ìŠ¤ ì •ì˜ (model.pyì˜ ì‹±ê¸€í†¤ íŒ¨í„´ ì¬ì‚¬ìš©)
class AdvancedConversationalRAG:
    def __init__(self, vectorstore, query_model_name="gpt-4o"):
        """
        Hugging Faceì— ì—…ë¡œë“œëœ fine-tuned ëª¨ë¸ì„ ì‚¬ìš©í•œ ëŒ€í™”í˜• RAG ì‹œìŠ¤í…œ
        Args:
            vectorstore: ë²¡í„° ì €ì¥ì†Œ
            query_model_name: ì¿¼ë¦¬ ì¬ì‘ì„±ìš© OpenAI ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: gpt-4o)
        """
        # historyì— ëŒ€í™” ì €ì¥
        self.history = []
        
        # ì¿¼ë¦¬ ì¬ìƒì„±ê¸° (OpenAI GPT ì‚¬ìš©)
        self.query_rewriter = AdvancedQueryRewriter(model_name=query_model_name)
        
        # ê°ê°ì˜ ê²€ìƒ‰ì–´ë¥¼ ë”°ë¡œ ê²€ìƒ‰í•œ ë’¤ì— ê²€ìƒ‰ê²°ê³¼ë¥¼ ì·¨í•©í•˜ëŠ” ë©€í‹°ì¿¼ë¦¬ ë¦¬íŠ¸ë¦¬ë²„
        self.retriever = MultiQueryRetriever(vectorstore=vectorstore, query_rewriter=self.query_rewriter)
        
        # ë‹µë³€ ìƒì„±ìš© ëª¨ë¸ ë¡œë“œ (model.pyì˜ ì‹±ê¸€í†¤ íŒ¨í„´ ì¬ì‚¬ìš©)
        print("âœ… RAG ì—”ì§„: model.pyì˜ ì‹±ê¸€í†¤ ëª¨ë¸ ì¬ì‚¬ìš©")
        self.llm, self.tokenizer = _load_model()
        self.device = self.llm.device
        print(f"âœ… RAG ëª¨ë¸ ì„¤ì • ì™„ë£Œ! Device: {self.device}")

        # ì‘ë‹µ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ì˜ì–´ ë²„ì „)
        self.response_template = """You are a professional psychologist specialized in HTP (House-Tree-Person) test interpretation.
Your role is to provide clear, professional psychological interpretations based on drawing features.

User Question: {query}

Please provide your interpretation based on the following reference information:
{context}

Guidelines:
1. If the user's question contains multiple queries, address each one clearly and separately.
2. Base your answer only on the provided information. If information is insufficient, honestly state that you don't know.
3. Provide your answer in Korean language.
4. If there are original sources in the provided information, cite them appropriately.
5. Explain possible psychological meanings in a professional manner.

Answer:"""
        
    def generate_response(self, prompt: str) -> str:
        """Fine-tuned ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„±"""
        print("=" * 80)
        print("ğŸ“ [RAG PROMPT] RAG í•´ì„ ìƒì„± í”„ë¡¬í”„íŠ¸:")
        print("-" * 80)
        print(prompt)
        print("=" * 80)
        
        # Qwen í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
        messages = [
            {"role": "system", "content": "You are a professional psychologist specialized in HTP test interpretation."},
            {"role": "user", "content": prompt}
        ]
        
        formatted_prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # í† í°í™” ë° ìƒì„± (ë””ë°”ì´ìŠ¤ ëª…ì‹œì  ì§€ì •)
        inputs = self.tokenizer(formatted_prompt, return_tensors="pt")
        
        # âœ… ëª¨ë“  ì…ë ¥ í…ì„œë¥¼ ëª¨ë¸ê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.llm.generate(
                **inputs,
                max_new_tokens=1024,
                temperature=0.3,
                do_sample=True,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        # ë””ì½”ë”© (ì…ë ¥ ë¶€ë¶„ ì œì™¸)
        response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        return response.strip()
        
    def query(self, current_query: str, category: str) -> Dict:
        # ê´€ë ¨ ë¬¸ì„œê²€ìƒ‰ (category íŒŒë¼ë¯¸í„° ì¶”ê°€)
        docs, rewritten_queries = self.retriever.retrieve(current_query, category)

        # ë¬¸ì„œ ë‚´ìš©ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if docs:
            context = "\n\n".join([f"ë¬¸ì„œ {i+1}:\n{doc.page_content}" for i, doc in enumerate(docs)])
            formatted_prompt = self.response_template.format(query=current_query, context=context)
        else:
            # ë¬¸ì„œ ì—†ìœ¼ë©´ ì¼ë°˜ ì§€ì‹ ê¸°ë°˜ ë‹µë³€ ìƒì„±
            formatted_prompt = f"User Question: {current_query}\n\nNo documents were retrieved, but please provide an appropriate answer based on your knowledge."

        # Fine-tuned LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        response = self.generate_response(formatted_prompt)

        # íˆìŠ¤í† ë¦¬ì— ì €ì¥
        record = {
            "user_query": current_query,
            "rewritten_queries": rewritten_queries,
            "retrieved_docs": [
                {"content": d.page_content, "metadata": d.metadata} for d in docs
            ],
            "final_answer": response
        }

        self.retriever.history.append(record)

        # ë¬¸ì„œ ë‚´ìš©ì„ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜)
        rag_docs = [doc.page_content for doc in docs]
        
        # ê²°ê³¼ ë°˜í™˜
        return {
            "query": current_query,
            "result": response,
            "rewritten_queries": rewritten_queries,
            "source_documents": docs,
            "rag_docs": rag_docs  # í”„ë¡ íŠ¸ì—”ë“œê°€ ì‚¬ìš©í•˜ëŠ” í•„ë“œ
        }
