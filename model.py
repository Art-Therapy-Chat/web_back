from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ í•œ ë²ˆë§Œ ë¡œë“œ
_model = None
_tokenizer = None
_model_name = "helena29/Qwen2.5_LoRA_for_HTP"

def _load_model():
    """ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œ (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    global _model, _tokenizer
    
    if _model is None:
        print(f"ğŸ”¥ Loading Qwen HTP Model: {_model_name}")
        print(f"ğŸ” CUDA Available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"ğŸ” CUDA Device: {torch.cuda.get_device_name(0)}")
            print(f"ğŸ” CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        _tokenizer = AutoTokenizer.from_pretrained(_model_name)
        
        # ëª¨ë¸ ë¡œë“œ (LoRA ì–´ëŒ‘í„°ê°€ ì´ë¯¸ ë³‘í•©ëœ ìƒíƒœ)
        _model = AutoModelForCausalLM.from_pretrained(
            _model_name,
            device_map="auto",
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
        )
        
        print(f"âœ… Qwen HTP Model loaded successfully!")
        print(f"âœ… Model Device: {_model.device}")
    
    return _model, _tokenizer


def generate_with_qwen(prompt: str):
    """
    Qwen ëª¨ë¸ì„ ì‚¬ìš©í•´ í…ìŠ¤íŠ¸ ìƒì„±
    ëª¨ë¸ì€ ìµœì´ˆ 1íšŒë§Œ ë¡œë“œë˜ê³  ì¬ì‚¬ìš©ë¨
    """
    # ëª¨ë¸ ë¡œë“œ (ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìœ¼ë©´ ì¬ì‚¬ìš©)
    model, tokenizer = _load_model()
    
    print("=" * 80)
    print("ğŸ“ [PROMPT] í•´ì„ ìƒì„± í”„ë¡¬í”„íŠ¸:")
    print("-" * 80)
    print(prompt)
    print("=" * 80)
    
    print(f"ğŸ” [generate_with_qwen] Model device: {model.device}")
    
    # ì…ë ¥ í…ì„œ ì¤€ë¹„
    inputs = tokenizer(prompt, return_tensors="pt")
    
    # ëª¨ë“  ì…ë ¥ì„ ëª¨ë¸ê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    print(f"ğŸ” [generate_with_qwen] Input device: {inputs['input_ids'].device}")
    
    # ìƒì„±
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    # í”„ë¡¬í”„íŠ¸ ì œê±°: ì…ë ¥ í† í° ì´í›„ë§Œ ì¶”ì¶œ
    input_len = inputs["input_ids"].shape[1]
    generated_ids = outputs[0][input_len:]
    
    result = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
    
    print(f"âœ… [generate_with_qwen] Generated {len(result)} characters")
    
    return result
