from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch


def generate_with_qwen(prompt: str):
    base = "Qwen/Qwen2.5-1.5B-Instruct"

    # í† í¬ë‚˜ì´ì €
    tokenizer = AutoTokenizer.from_pretrained(base)

    # base model
    model = AutoModelForCausalLM.from_pretrained(
        base,
        device_map="auto",
        torch_dtype="auto"
    )

    # LoRA ì ìš©
    peft_model = PeftModel.from_pretrained(
        model,
        "./data/adapter_model",
        adapter_name="qwen_lora"
    )

    # ì…ë ¥ í…ì„œ ì¤€ë¹„
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # ìƒì„±
    outputs = peft_model.generate(
        **inputs,
        max_new_tokens=200,
        temperature=0.7
    )

    # -------------------------------
    # ğŸ”¥ í”„ë¡¬í”„íŠ¸ ì œê±°: ì…ë ¥ í† í° ì´í›„ë§Œ ì¶”ì¶œ
    # -------------------------------
    input_len = inputs["input_ids"].shape[1]
    generated_ids = outputs[0][input_len:]

    result = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()

    # ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ
    del model
    torch.cuda.empty_cache()

    return result
