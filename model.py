from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ í•œ ë²ˆë§Œ ë¡œë“œ
_model = None
_tokenizer = None
_model_name = "helena29/Qwen2.5_LoRA_for_HTP"

def _load_model():
    """ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œ (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    global _model, _tokenizer
    
    if _model is None:
        print(f"ğŸ”¥ Loading Qwen HTP Model: {_model_name}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        _tokenizer = AutoTokenizer.from_pretrained(_model_name)
        
        # ëª¨ë¸ ë¡œë“œ (LoRA ì–´ëŒ‘í„°ê°€ ì´ë¯¸ ë³‘í•©ëœ ìƒíƒœ)
        _model = AutoModelForCausalLM.from_pretrained(
            _model_name,
            device_map="auto",
            torch_dtype="auto"
        )
        
        print("âœ… Qwen HTP Model loaded successfully!")
    
    return _model, _tokenizer


def generate_with_qwen(prompt: str):
    """
    Qwen ëª¨ë¸ì„ ì‚¬ìš©í•´ í…ìŠ¤íŠ¸ ìƒì„±
    ëª¨ë¸ì€ ìµœì´ˆ 1íšŒë§Œ ë¡œë“œë˜ê³  ì¬ì‚¬ìš©ë¨
    """
    # ëª¨ë¸ ë¡œë“œ (ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìœ¼ë©´ ì¬ì‚¬ìš©)
    model, tokenizer = _load_model()
    
    # ì…ë ¥ í…ì„œ ì¤€ë¹„
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    # ìƒì„±
    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        temperature=0.7
    )
    
    # í”„ë¡¬í”„íŠ¸ ì œê±°: ì…ë ¥ í† í° ì´í›„ë§Œ ì¶”ì¶œ
    input_len = inputs["input_ids"].shape[1]
    generated_ids = outputs[0][input_len:]
    
    result = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
    
    return result
